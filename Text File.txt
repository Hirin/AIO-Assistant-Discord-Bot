Nguyen Thi Mai Loan
09:38
dáº¡ rÃµ áº¡

Hakabi
09:45
ChÃ o ad, Nghe rÃµ áº¡

Dung Trieu
09:48
oke a.

Jason Nguyá»…n
09:51
Dáº¡ rÃµ áº¡

LÃª Há»“ Anh Duy
11:17
normalization

Jason Nguyá»…n
11:29
Pooling

Nguyá»…n Tuáº¥n Minh
11:29
batch normalization, skip connection, input normalization,

VÃµ HÆ°Æ¡ng
11:36
skip connection

he initialization, tÄƒng layers

LÃª Há»“ Anh Duy
12:36
thay Ä‘á»•i hÃ m kÃ­ch hoáº¡t áº¡?

VÃµ HÆ°Æ¡ng
12:57
giáº£m lr

Äá»“ng Sdr
13:29
Convolution

HuÃ¢n Pháº¡m
13:49
Optimizer, Shortcut Back

Anh ThÆ°
13:55
train tá»«ng layer

Táº¥n Äáº¡t
14:15
Ä‘á»•i phÆ°Æ¡ng phÃ¡p optimizer

NgÃ´ Minh Duy
15:47
Normalize Gradient

Dennis Nguyá»…n
37:30
2

Äáº¡t Thanh LÃª
37:31
2

TÃº TrÃ¢n
37:33
2

Äá»©c XuÃ¢n
37:34
2

Jason Nguyá»…n
41:49
Side note: á» ngoÃ i thá»±c táº¿, á»Ÿ cÃ¡c cÃ´ng ty Ä‘a quá»‘c gia mÃ  cÃ³ lÃ m bÃ i toÃ¡n vá» CV; thÃ¬ trÆ°á»›c khi áº£nh Ä‘Æ°á»£c dÃ¹ng lÃ m data thÃ¬ pháº£i thÃ´ng qua ráº¥t nhiá»u test vá» cháº¥t lÆ°á»£ng áº£nh

Collapse All
ğŸ‘
1
â¤ï¸
2
Jason Nguyá»…n
42:32
CÃ¡i nÃ y FoxConn, ABB, General Electric hay Siemens Ã¡p dá»¥ng áº¡

KhÃ¡nh KDN
46:06
Gáº·p Samples kiá»ƒu VN bá»‹ máº¥y biá»ƒn quáº£ng cÃ¡o, cÃ nh cÃ¢y che thÃ¬  khÃ³c thÃ©t :))

Collapse All
XuÃ¢n CÆ°á»ng
46:47
tháº¿ mÃ  camera AI soi rÃµ Ä‘en Ä‘Ã©t, ná»ƒ team nÃ o lÃ m cÃ¡i Ä‘Ã³ kkk

ğŸ˜‚
1
ğŸ¤£
1
Nguyá»…n Minh QuÃ¢n
47:11
@XuÃ¢n CÆ°á»ng do cháº¥t lÆ°á»£ng camera ná»¯a mÃ 

Diá»‡p Báº£o CÆ°á»ng
48:06
Ã­t hÃ´m lÃ m model tháº¥y biá»ƒn bÃ¡o nÃ o nÃºp nÃºp cÃ nh cÃ¢y thÃ¬ cáº£nh bÃ¡o xÃ¡c suáº¥t pikachu, xong Ä‘Æ°á»£c lÃªn phÆ°á»ng ğŸ¥²

Báº£o VÄƒn
47:58
1

SÆ¡n Äá»—
48:04
1

Dennis Nguyá»…n
48:04
1

TÃº TrÃ¢n
48:04
2

LÃª Há»“ Anh Duy
48:04
1

Jason Nguyá»…n
48:05
1 áº¡

Nguyá»…n Minh QuÃ¢n
48:07
1

XuÃ¢n CÆ°á»ng
48:08
1

NhÃ¢n Nguyá»…n Äá»©c
48:10
1

VÃµ HÆ°Æ¡ng
48:12
1 áº¡

Phuc Dinh Tran Hoang
48:12
1

Äá»“ng Sdr
48:14
1

Viet Hoang
48:15
1

Äáº¡t Thanh LÃª
48:15
1

HoÃ ng Kiá»‡t
49:30
náº¿u Ä‘á»™ chÃ­nh xÃ¡c cá»§a test > train

váº«n Ä‘Æ°á»£c cháº¥p nháº­n pháº£i khÃ´ng ad

Vu Tuan Anh
50:23
Viá»‡c thay Ä‘á»•i tá»‰ lá»‡ xÃ¡c suáº¥t 75% sang giÃ¡ trá»‹ khÃ¡c áº£nh hÆ°á»Ÿng gÃ¬ khÃ´ng áº¡?

Vu Tuan Anh
51:44
Cáº£m Æ¡n ad (gÃ­a trá»‹ thá»±c nghiá»‡m)

NgÃ´ Minh Duy
52:51
ad Æ¡i cho em há»i , náº¿u dataset cá»§a mÃ¬nh lÃ  50k áº£nh, thÃªm nhiá»…u thÃ¬ data set má»›i sáº½ lÃ  100k (50k bth + 50k cÃ³ nhiá»…u) hay váº«n chá»‰ 50K nhiá»…u thÃ´i áº¡

Collapse All
Jason Nguyá»…n
53:09
Tuá»³ vÃ o lá»±a chá»n cá»§a báº¡n

Báº£o VÄƒn
53:33 (Edited)
dáº¡ dáº¡ng nhÆ° dÃ¹ng BN thÃ¬ sáº½ Ä‘Æ°a Ä‘á» thi thá»±c táº¿ vá» láº¡i dáº¡ng gáº§n gáº§n giá»‘ng dáº¡ng Ã´n tÃ¢p =))

ğŸ‘
1
Nguyen Thien Quang
54:06
Bn á»•n Ä‘á»‹nh distribution cá»§a giÃ¡ trá»‹ train

Nguyá»…n Minh QuÃ¢n
54:15
BN Ä‘Æ°a dá»¯ liá»‡u thÃ nh 1 khoáº£ng quy Ä‘á»‹nh trÆ°á»›c dá»… há»c, dá»… thi

Táº¥n Äáº¡t
54:16
batch normalazation giÃºp chuáº©n hÃ³a tá»«ng feature map giÃºp trÃ¡nh bá»‹ shift distribution qua tá»«ng kernel

ğŸ‘
2
HoÃ n NT
01:04:11
Nhiá»u chá»— há» nÃ³i lÃ  náº¿u ko cÃ³ bÆ°á»›c scale vÃ  shift thÃ¬ ko tá»‘t báº±ng viá»‡c thay Ä‘á»•i láº¡i phÃ¢n phá»‘i cá»§a má»—i kÃªnh , má»—i cÃ¡i má»™t kiá»ƒu hoÃ n toÃ n khÃ¡c nhau, ko thá»ƒ Ä‘oÃ¡n Ä‘c nÃ³ sáº½ ntn. E chÆ°a hiá»ƒu láº¯m chá»— nÃ y áº¡. Náº¿u ko cÃ³ shift vÃ  scale thÃ¬ bÆ°á»›c batch norm nÃ o cug sáº½ lÃ  chuáº©n hoÃ¡ vá» 0, 1, vÃ  BN ko cÃ³ tham sá»‘ há»c, nhÆ° tháº¿ cÃ³ pháº£i tá»‘t hÆ¡n ko ah

Dennis Nguyá»…n
01:08:18
Giáº£m sá»± phá»¥ thuá»™c weight a?

Nguyá»…n Minh QuÃ¢n
01:09:54
khÃ¡c

LÃª Há»“ Anh Duy
01:09:55
khÃ¡c

VÃµ HÆ°Æ¡ng
01:09:56
khÃ¡c áº¡

HuÃ¢n Pháº¡m
01:10:52
KhÃ´ng áº¡

Nguyá»…n Minh QuÃ¢n
01:10:53
khÃ¡c láº¡\

Äá»“ng Sdr
01:10:53
khÃ¡c

Quang Nguyen Dang
01:10:54
khac

VÃµ HÆ°Æ¡ng
01:10:55
khÃ´ng áº¡

XuÃ¢n CÆ°á»ng
01:10:55
khong

Dennis Nguyá»…n
01:12:29
ko

XuÃ¢n CÆ°á»ng
01:12:33
ko

Ngoc Anh Ngo
01:13:54
ThÃªm nhiá»…u áº¡

ğŸ‘
1
Äá»“ng Sdr
01:21:08
ok

Thanh Ha Nguyen
01:21:08
dáº¡ oke áº¡

Táº¥n Äáº¡t
01:21:09
1

Dennis Nguyá»…n
01:21:09
1

25 - Äáº·ng Nháº­t Minh
01:21:11
1

SÆ¡n Äá»—
01:21:12
1

Nguyá»…n Tuáº¥n Minh
01:34:37
Dropout cÅ©ng lÃ  1 dáº¡ng thÃªm nhiá»…u áº¡ táº¡i mÃ´ hÃ¬nh bá»‹ máº¥t thÃ´ng tin áº¡ : D

ğŸ‘
1
Nguyen Thien Quang
01:34:42
Giam overfit

VÃµ HÆ°Æ¡ng
01:35:31
giá»‘ng nhÆ° bá» Ä‘i nhá»¯ng cÃ¢u quÃ¡ khÃ³ trong dá» Ã´n táº­p áº¡?

Dennis Nguyá»…n
01:35:44
Giáº£m bá»›t thÃ´ng tin khÃ´ng cáº§n thiáº¿t vÃ  khuáº¿ch Ä‘áº¡i cÃ¡c thÃ´ng tin cÃ²n láº¡i thÃ¬ mÃ´ hÃ¬nh há»c tá»‘t hÆ¡n áº¡

â¤ï¸
2
Quang Nguyen Dang
01:36:23
Äá»ƒ trÃ¡nh cÃ¡c nodes há»c thuá»™c vÃ  phá»¥ thuá»™c láº«n nhau

â¤ï¸
1
NgÃ´ Minh Duy
01:37:52
Em chá»‰ má»›i tÃ¬m hiá»ƒu sÆ¡ qua SNN , phÆ°Æ¡ng phÃ¡p Dropout nÃ y hiá»‡n cÃ³ Ä‘ang cÃ¹ng Ã­ tÆ°á»Ÿng vá»›i SNN hong áº¡

â¤ï¸
1
XuÃ¢n CÆ°á»ng
01:38:00
nhÆ°ng mÃ  náº¿u tháº¿ thÃ¬ táº¡i sao thÃªm nhiá»…u báº±ng nhiá»u cÃ¡ch  (noise, batch norm, dropout) thÃ¬ láº¡i tá»‘t hÆ¡n lÃ  dÃ¹ng 1 cÃ¡ch ğŸ˜„

Collapse All
Jason Nguyá»…n
01:38:22
KhÃ´ng cÃ³ 1 cá»¡ giÃ y cho má»i cá»¡ chÃ¢n

Dennis Nguyá»…n
01:38:26
drop out thÃ¬ nÃªn Ä‘á»ƒ bao nhiÃªu áº¡? Hay vá»›i má»—i loáº¡i dá»¯ lieu thÃ¬ sáº½ pháº£i cháº¡y nhiá»u láº§n Ä‘á»ƒ tÃ¬m ra áº¡?

25 - Äáº·ng Nháº­t Minh
01:38:34
CÃ³ khi nÃ o xáº£y ra tÃ¬nh tráº¡ng dropout á»Ÿ má»—i layer mÃ  Ä‘áº¿n output cuá»‘i pÃ m cho mÃ´ hÃ¬nh chá»‰ há»c Ä‘c 10% ko áº¡

Hakabi
01:39:39
sá»‘ lÆ°á»£ng thÃªm nhiá»…u lÃ  mÃ¬nh táº¡o bao nhiÃªu phiÃªn báº£n nhiá»…u cho táº¥m áº£nh data gá»‘c Ä‘á»ƒ mÃ´ hÃ¬nh há»c ? VÃ  má»©c Ä‘á»™ nhiá»…u nhiá»u hay Ã­t mÃ¬nh sáº½ kiá»ƒm soÃ¡t tháº¿ nÃ o áº¡, cÃ³ ngÆ°á»¡ng set nhiá»…u ra sao khÃ´ng ?

Äáº¡t Thanh LÃª
01:40:08 (Edited)
E tháº¥y paper vgg há» Ä‘á»ƒ 0.5

Thanh Ha Nguyen
01:40:13
dropout nÃªn Ä‘á»ƒ sau pooling pháº£i khÃ´ng tháº§y? Hay cÃ³ nguyÃªn táº¯c Ä‘á»ƒ dropout trong layer khÃ´ng áº¡?

HoÃ n NT
01:42:19
NhÆ° váº­y lÃ  lá»i giáº£i thÃ­ch : lÃ m cho cÃ¡c neuron Ä‘á»¡ phá»¥ thuá»™c láº«n nhau, lÃ  ko Ä‘á»§ thuyáº¿t phá»¥c pháº£i ko ah. DÆ°á»ng nhÆ° chá»‰ cÃ³ lá»i giáº£i thÃ­ch ThÃªm nhiá»u lÃ  rÃµ rÃ ng nháº¥t. E nghÄ© cÃ¡i chá»‘ng phá»¥ thuá»™c láº«n nhau hÃ¬nh nhÆ° cug cÃ³ váº» lÃ  hÆ°á»›ng giáº£i thÃ­ch

VÅ© Kháº£i
01:43:02
dropout vs hÃ m relu hay dÃ¹ng cÃ³ lÃ m tÄƒng nguy cÆ¡ vanishing gradient khÃ´ng áº¡, vÃ¬ e nghÄ© khi Ä‘áº¡o hÃ m vá» khÃ¡ nhiá»u node sáº½ báº±ng 0 vÃ  khÃ´ng Ä‘Æ°á»£c cáº­p nháº­p

Collapse All
Äáº¡t Thanh LÃª
01:45:34
Theo mÃ¬nh Ä‘Ã³ lÃ  lÃ½ do há» dÃ¹ng batch norm Ã¡

Viet Hoang
01:43:14
Khi nÃ o mÃ¬nh sáº½ sá»­ dá»¥ng dropout áº¡.

Collapse All
ğŸ˜
2
ğŸ˜‡
1
Äá»“ng Sdr
01:47:42
Má»¥c tiÃªu cá»§a buá»•i há»c hÃ´m nay

Vu Tuan Anh
01:47:42
ÄÃ¢y lÃ  má»¥c tiÃªu chÃ­nh cá»§a bÃ i hÃ´m nay

HoÃ ng Kiá»‡t
01:49:38
em nghÄ© khÃ´ng

Nguyá»…n Minh QuÃ¢n
01:49:54
Ä‘Ã£ Ä‘á»ƒ trong mÃ´ hÃ¬nh thÃ¬ e nghÄ© lÃ  cÃ³

Van Hau Nguyen
01:50:08
CÃ³ vÃ  khÃ´ng

Äá»“ng Sdr
01:50:32
Giá»‘ng

Quang Nguyen
01:51:18
KhÃ¡c: 

train: bá» bá»›t dá»¯ liá»‡u 

Test: thÃªm nhiá»…u cho khÃ³ hÆ¡n

HoÃ ng Kiá»‡t
01:51:18
em gáº·p trÆ°á»ng há»£p sá»­ dá»¥ng dropout do nÃ³ random  khi train khoáº£ng  1 2 epoch thÃ¬ val > train, thÃ¬ cÃ³ cháº¥p nháº­n hay máº¥t cÃ¢n báº±ng khÃ´ng ad

Hakabi
01:51:44
thá»±c táº¿, cÃ³ trÆ°á»ng há»£p nÃ o mÃ¬nh dÃ¹ng droput mÃ£i mÃ  nÃ³ khÃ´ng lÃªn Ä‘Æ°á»£c acc cho táº­p test khÃ´ng nhá»‰ ğŸ˜„ ?

Vu Tuan Anh
01:51:49
Test khÃ´ng Drop out Theo em hiá»ƒu. Chá»‰ Drop out trÃªn train

Nguyá»…n Tuáº¥n Minh
01:53:19
táº¡i model bá»‹ máº¥t thÃ´ng tin áº¡

Táº¥n Äáº¡t
01:53:19
model bá»‹ há»c khÃ³ hÆ¡n do bá»‹ nhiá»…u

LÃª Há»“ Anh Duy
01:53:20
táº­p train há»c khÃ³ hÆ¡n áº¡?

Vu Dang
01:58:52
dáº¡ cÃ²n

Dung Trieu
01:58:52
oke a

VÃµ HÆ°Æ¡ng
01:58:53
ok áº¡

KhÃ¡nh KDN
01:58:53
dáº¡ cÃ²n

XuÃ¢n CÆ°á»ng
01:58:59
cÃ²n nguyÃªn áº¡

Jason Nguyá»…n
02:00:10
VÃ¬ sao chá»‰ cÃ³ L2 regularization khÃ´ng cÃ³ L3, L4 hay Ln áº¡?

HoÃ n NT
02:02:56
Ah, chá»— Dropout mÃ  chá»‘ng phá»¥ thuá»™c láº«n nhau lÃ  1 lÃ½ do cÃ³ thá»ƒ giáº£i thÃ­ch ntn Ä‘c ko ah : trong cÃ¡c bÃ i cÆ¡ báº£n, thÆ°á»ng bao h cÅ©ng pháº£i cÃ³ bÆ°á»›c Feature Engineering, lÃ m cho cÃ¡c feature tÃ¡ch biá»‡t nhau thÃ¬ má»›i hiá»‡u nÄƒng tá»‘t Ä‘c, vd thá»±c hiá»‡n cÃ¡c bÆ°á»£vs lÃ m cho tá»«ng feature Ä‘á»™c láº­p, hay lÃ  PCA, â€¦. Váº­y dropdout nÃ³ cug lÃ m cho feature trá»Ÿ nÃªn Ä‘á»™c láº­p, nÃªn lÃ  Ä‘Æ°á»ng cam sáº½ lÃªn

HoÃ n NT
02:06:59 (Edited)
(covariance giá»¯a cÃ¡c feature cÃ ng Ã­t cÃ ng tá»‘t)

Nguyá»…n Tuáº¥n Minh
02:12:12
Váº­y thÃªm dá»¯ liá»‡u lá»›n cÅ©ng lÃ  Ã½ tÆ°á»Ÿng cÃ³ transfer learning Ä‘Ãºng khÃ´ng áº¡. Khi mÃ  mÃ¬nh cÃ³ 1 mÃ´ hÃ¬nh Ä‘Ã£ pretrained tá»« 1 táº­p dá»¯ liá»‡u ráº¥t lá»›n...

Nguyá»…n Äá»©c
02:14:50
há»c khÃ³ hÆ¡n nÃªn thi tá»‘t hÆ¡n :))

HuÃ¢n Pháº¡m
02:14:52
Há»c nhiá»u quÃ¡ nhÆ°ng phÆ°Æ¡ng phÃ¡p chÆ°a quÃ¡ máº¡nh áº¡ =))

XuÃ¢n CÆ°á»ng
02:14:52
thÃªm data nhiá»…u nÃªn model pháº£i há»c thÃªm

Nguyen Thien Quang
02:14:53
dá»¯ lieu train khÃ³ hÆ¡n

VÃµ HÆ°Æ¡ng
02:14:53
model há»c khÃ³ hÆ¡n do cÃ³ nhiá»…u áº¡

Äáº¡t Thanh LÃª
02:15:01
há»c nhiá»u hÆ¡n :>

Äá»©c XuÃ¢n
02:16:41
tÄƒng epoch lÃªn áº¡?

Nguyá»…n Minh QuÃ¢n
02:16:43
cÃ²n kháº£ nÄƒng há»c thÃªm

Báº£o VÄƒn
02:16:44
tÄƒng layer

Nguyá»…n Tuáº¥n Minh
02:16:44
tÄƒng capacity cá»§a model áº¡

VÃµ HÆ°Æ¡ng
02:16:44
cáº§n tÄƒng train acc

Van Hau Nguyen
02:18:10
TÃ´is Æ°u hyper parameter

Nguyá»…n Minh QuÃ¢n
02:18:45
initialization

Há»c Váº¹t
02:18:46
Skip Connection

Táº¥n Äáº¡t
02:18:47
skip connection

VÃµ HÆ°Æ¡ng
02:18:50
skip connection, advanced initialization

Táº¥n Äáº¡t
02:20:27
b

Äá»“ng Sdr
02:20:28
 b>

XuÃ¢n CÆ°á»ng
02:20:29
|b|>|a|

Van Hau Nguyen
02:21:11
K phÃ¹ há»£p

LÃª Há»“ Anh Duy
02:21:12
ko áº¡

Nguyá»…n Tuáº¥n Minh
02:21:17
khÃ´ng áº¡

Nguyen Thien Quang
02:21:18
ko áº¡

VÃµ HÆ°Æ¡ng
02:21:19
ko áº¡

Há»c Váº¹t
02:21:22
Adaptive Learning

KhÃ¡nh KDN
02:21:26
giáº£m

Nguyá»…n Tuáº¥n Minh
02:21:28
giáº£m áº¡

Nguyá»…n Minh QuÃ¢n
02:21:29
giáº£m

Van Hau Nguyen
02:21:31
Linh Ä‘á»™ng

Nam Khanh
02:31:13
oh váº­y lÃ  trick tá»•ng thá»ƒ lÃ  thá»­ nghiá»‡m trick 1 Ä‘áº¿n 4 trc xong náº¿u muá»‘n nhu cáº§u tÄƒng test thÃ¬ pháº£i tÄƒng train lÃªn hÆ¡n thÃ¬ Ã¡p dá»¥ng trick 5,6 thÃ¬ sáº½ bá»• trá»£ tÄƒng accuracy train thÃ¬ Ä‘áº¿n h mÃ¬nh mong muá»‘n cÃ³ thá»ƒ quay láº¡i bá»• sung trick 1-4 Ä‘áº¿n khi accuracy Ä‘áº¡t Ä‘á»™ phÃ¹ há»£p háº£ ad áº¡, vÃ¬ e tháº¥y trick 5 thÃªm dá»¯ liá»‡u hÆ¡n thÃ¬ accuracy test tÄƒng cÃ²n trick 6 lÃ  train tÄƒng

ğŸ‘
2
AI VIET NAM
02:43:07
https://kahoot.it/?pin=2821508&refer_method=link

Báº£o VÄƒn
02:57:54
21

Äá»“ng Sdr
02:57:55
21

VÃµ HÆ°Æ¡ng
02:57:55
21

LÃ¢n Kim ViÃªn
03:03:00
Láº§n Ä‘áº§u lÃªn top 1!

ğŸ‘
1
â¤ï¸
1
ğŸ‘
1
Hakabi
03:14:45
Em muá»‘n há»i láº¡i á»Ÿ pháº§n Quiz cÃ¢u sá»‘ 5. Theo em nghÄ© lÃ  náº¿u mÃ¬nh lÃ m augmentation cho táº­p test thÃ¬ cÅ©ng tá»‘t chá»© nhá»‰ ? VÃ¬ khi data á»Ÿ test nháº­n diá»‡n Ä‘Æ°á»£c nhiá»u pattern, nhiá»u noise hÆ¡n thÃ¬ viá»‡c Ä‘Æ°a data cÃ³ pattern/nhiá»…u khÃ¡c Ä‘i chÃºt so vá»›i data train cÅ©ng sáº½ giÃºp mÃ´ hÃ¬nh nhá»› Ä‘áº¿n pattern gá»‘c cá»§a táº­p train vÃ  phÃ¢n loáº¡i Ä‘Ãºng theo táº­p train?

Collapse All
Nguyá»…n Äá»©c
03:16:23
nhÆ°ng táº­p test mÃ¬nh Ä‘Ã¢u cáº§n há»c hay cáº­p nháº­t tham sá»‘ Ä‘Ã¢u áº¡, vÃ  trong thá»±c táº¿ nhÆ° ad cÅ©ng Ä‘Ã£ mÃ´ phá»ng há»“i nÃ£y lÃ  video giao thÃ´ng thÃ¬ nÃ³ cÅ©ng Ä‘Ã£ cÃ³ sáºµn cÃ¡c nhiá»…u/má», lá»‡ch Ã¡nh sÃ¡ng mÃ u sáº¯c rá»“i

Báº£o VÄƒn
03:19:06
nÄƒm sau cÃ¡c pháº§n giá»‘ng nhÆ° cÃ¡c cuá»™c thi zaloAI, vnptAI thÃ¬ AIO2025 cÃ³ Ä‘Æ°á»£c join ko áº¡

Collapse All
ğŸ‘ğŸ¿
1
Äá»“ng Sdr
03:20:01
Welcom nha b

Nguyá»…n Äá»©c
03:20:00
sá»± tháº­t vÃ©n mÃ n...

Nguyá»…n Minh QuÃ¢n
03:20:06
gáº·p cÃ¡c tá»• chá»©c khÃ¡c cháº¯c cho out luÃ´n ğŸ™‚)

NgÃ´ Minh Duy
03:21:25
ad Æ¡i cho em há»i bÃªn ngoÃ i má»™t tÃ­ áº¡. Em cÃ³ Ä‘áº·t má»™t cÃ¢u há»i hiá»‡n CNN cÃ³ nhÆ°á»£c Ä‘iá»ƒm gÃ¬ mÃ  chÆ°a báº±ng vá»›i há»‡ tháº§n kinh cá»§a con ngÆ°á»i =)) (em trÃ¡i ngÃ nh nÃªn Ä‘Ã´i khi há»i nhá»¯ng cÃ¢u há»i hÆ¡i ngá»‘ ) . ThÃ¬ vÃ´ tÃ¬nh em tÃ¬m Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh SNN (Spiking Neural Network). hiá»‡n theo kn cá»§a ad mÃ´ hÃ¬nh nÃ y cÃ³ phá»• biáº¿n Ä‘á»ƒ Ä‘Ã o sÃ¢u research ko áº¡. SNN khÃ¡c vá»›i CNN á»Ÿ chá»— thay vÃ¬ liÃªn tá»¥c truyá»n giÃ¡ trá»‹ qua cÃ¡c node, thÃ¬ trong SNN chá»‰ thá»±c hiá»‡n truyá»n dá»¯ liá»‡u khi giÃ¡ trá»‹ (spike) táº¡i má»™t node vÆ°á»£t má»™t ngÆ°á»¡ng threshold nÃ o Ä‘Ã³ (mÃ´ phá»ng quÃ¡ trÃ¬nh truyá»n tÃ­n hiá»‡u giá»¯a cÃ¡i neuron sinh há»c trong nÃ£o ng)

Phong Ha
03:21:25
Em cÃ²n yáº¿u muá»‘n tham gia cÃ¡c cuá»™c thi thÃ¬ join ntn áº¡, chá»§ yáº¿u láº¥y kiáº¿n thá»©c thui

Há»c Váº¹t
03:24:15
ILLYA SUTKESVER

Äá»©c XuÃ¢n
03:27:56
Em out 3 cÃ¡i luÃ´n rÃ¹i ad

HoÃ n NT
03:29:08
Ad tráº£ lá»i cÃ¢u há»i lÃºc Ä‘áº§u vá» BatchNorm ah, cháº¯c nÃ³ trÃ´i rá»“i

HoÃ n NT
03:30:22
Ko ah

Táº¡i sao láº¡i cÃ³ scale shift, bá» nÃ³ Ä‘i cÃ³ pháº£i tá»‘t hÆ¡n ko ah

VÃ¬ nhiá»u ngÆ°á»i nÃ³i pháº£i Ä‘á»ƒ cho nÃ³ phÃ¢n bá»‘ láº¡i lung tung ko thá»ƒ Ä‘oÃ¡n Ä‘c thÃ¬ má»›i tá»‘t

Thanh Ha Nguyen
03:34:50
Dáº¡ gamma vs Beta Ä‘Ã³ cÅ©ng Ä‘Æ°á»£c cáº­p nháº­t theo hÃ m loss cá»§a bÃ i toÃ¡n luÃ´n háº£ ad

HoÃ n NT
03:35:47
NhÆ°ng dÃ¹ng BN rÃµ rÃ ng nÃ³ tá»‘t hÆ¡n ko dÃ¹ng ah, nÃªn e há»i táº¡i sao nÃ³ tá»‘t khi cÃ³ cáº£ scale, shift ah

HoÃ n NT
03:39:13
Ã cá»§a e lÃ  cÃ¡c tham sá»‘ scale shift Ä‘Ã³ Ä‘c Ä‘iá»u chá»‰nh theo má»¥c Ä‘Ã­ch cuá»‘i cÃ¹ng lÃ  hÃ m Loss tá»‘i thiá»ƒu, vÃ  máº¥y cÃ¡i scale shuft há»™i tá»¥ vá» cÃ¡i gÃ¬ Ä‘Ã³, vÃ  e tháº¥y giÃ¡ trá»‹ há»™i tá»¥ Ä‘Ã³ cÃ³ ko explainable láº¯m.

Shift

KhÃ¡nh KDN
03:40:37
CÃ³ tÃ­nh kháº£ thi :))

Äá»©c XuÃ¢n
03:44:32
Em tham gia thi nhÆ° black box nhma cÃ³ thÃªm kinh nghiá»‡m rÃ¹i áº¡

Thanh Ha Nguyen
03:45:45
Cáº£m Æ¡n Ad

Dung Trieu
03:46:16
em cáº£m Æ¡n ad

KhÃ¡nh KDN
03:46:17
dáº¡, cáº£m Æ¡n ad nhiá»u.

VÃµ HÆ°Æ¡ng
03:46:21
em cáº£m Æ¡n ad áº¡

Dennis Nguyá»…n
03:46:35
Bye ad áº¡

LÃ¢n Kim ViÃªn
03:46:37
Em cáº£m Æ¡n tháº§y!

Nguyá»…n Minh QuÃ¢n
03:46:42
Dáº¡ e cÃ¡m Æ¡n ad

LÃ¢n Kim ViÃªn
03:46:44
Em chÃ o tháº§y!

